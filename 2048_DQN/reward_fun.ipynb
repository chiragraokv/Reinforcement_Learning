{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fed8ca1",
   "metadata": {},
   "source": [
    "# One hot encodeing values of the game \n",
    "This is a better way to reoresent the game and using CNN also helps model learn better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2de617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laserhammer/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from game_2048 import game_2048\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd19eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Small_neural_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Small_neural_net,self).__init__()\n",
    "        #input is 16x4x4\n",
    "        self.conv1 = nn.Conv2d(16, 32, kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2, stride=1)\n",
    "        self.droupout1 = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(64, 200, kernel_size=2, stride=1)\n",
    "        self.droupout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        #200 x 1 x 1\n",
    "\n",
    "        self.deep1 = nn.Linear(200,128)\n",
    "        self.deep2 = nn.Linear(128,32)\n",
    "        self.deep3 = nn.Linear(32,4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.droupout1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.droupout2(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = F.relu(self.deep1(x))\n",
    "        x = F.relu(self.deep2(x))\n",
    "        return self.deep3(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37759a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_state(matrix):\n",
    "    encoded = np.zeros((16, 4, 4), dtype=np.float32)\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            val = matrix[i][j]\n",
    "            if val == 0:\n",
    "                encoded[0, i, j] = 1\n",
    "            else:\n",
    "                power = int(np.log2(val))\n",
    "                encoded[power, i, j] = 1\n",
    "    \n",
    "    return torch.tensor(encoded,dtype= torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "def eta_greedy(greedy,action_values):\n",
    "    action_values = action_values.tolist()\n",
    "    actions = [0,1,2,3]\n",
    "    if random.random()< greedy:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        max_val = max(action_values)\n",
    "        max_indices = [i for i, v in enumerate(action_values) if v == max_val]\n",
    "        return random.choice(max_indices)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a477593",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 0\n",
    "runs = 1000\n",
    "model = Small_neural_net()\n",
    "criteriation = nn.MSELoss()\n",
    "game = game_2048()\n",
    "buffer_game = game_2048()\n",
    "model_eval = copy.deepcopy(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "gamma = 0.99\n",
    "replay_memory = []\n",
    "action = [0,1,2,3]\n",
    "loss_his = []\n",
    "reward_his = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2930b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the buffer\n",
    "for i in range(100):\n",
    "        #initialise the replay memory\n",
    "        #run the model with copy to buffer up memory\n",
    "        s_t = buffer_game.matrix.copy()\n",
    "        a_t = random.choice(action)\n",
    "        r_t = buffer_game.run(a_t)\n",
    "        if(r_t == 0):\n",
    "            r_t = -10\n",
    "        if r_t == -1:\n",
    "            r_t = -20\n",
    "        s_t_1 = buffer_game.matrix.copy()\n",
    "        #store the replay in the memory\n",
    "        replay_memory.append([s_t,a_t,r_t,s_t_1])\n",
    "        if r_t == -1:\n",
    "            buffer_game.initialise()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4252fb5",
   "metadata": {},
   "source": [
    "# Using stocastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d5b16",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "batch_predict= []\n",
    "batch_target = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e37e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 427/1000 [07:18<2:36:01, 16.34s/it]"
     ]
    }
   ],
   "source": [
    "for run in tqdm(range(runs)):\n",
    "    r_h =  []\n",
    "    step = 0\n",
    "    flag = 0\n",
    "    game.initialise()\n",
    "    while flag != -1:\n",
    "        #train the model\n",
    "        s_t = game.matrix.copy()\n",
    "        #predict action value function\n",
    "        q_t = model(encode_state(s_t))\n",
    "        #choose an action based on action value functions predicted (throught eta greedy)\n",
    "        a_t = eta_greedy(0.2,q_t)\n",
    "        r_t = game.run(a_t)\n",
    "        # override reward\n",
    "        if(r_t == 0):\n",
    "            r_t = -10\n",
    "        if r_t == -1:\n",
    "            r_t = -20\n",
    "        \n",
    "        s_t_1 = game.matrix.copy()\n",
    "        #save the the experience in buffer \n",
    "        replay_memory.pop()\n",
    "        replay_memory.append([s_t,a_t,r_t,s_t_1])\n",
    "\n",
    "        #training\n",
    "        #sample mini batch from replay\n",
    "        experience = random.choice(replay_memory)\n",
    "        if(step%2 == 0):\n",
    "            model_eval = copy.deepcopy(model)\n",
    "        with torch.no_grad():\n",
    "            s_m,a_m,r_m,s_m_1 = experience\n",
    "            q_t_1 = model_eval(encode_state(s_m_1))\n",
    "            q_max = max(q_t_1[0])\n",
    "            # if the next state in experience is terminal then there is no future reward\n",
    "            # in this game to indicate terminal sate the reward outputs -1 \n",
    "            # (this has to be treated as a flag and not a reward hence the override with 0)\n",
    "            if(r_m == -20):\n",
    "                y = -20\n",
    "            else:\n",
    "                y = r_m+ gamma*q_max\n",
    "        # now that the predicted value has been calculated train the model\n",
    "        #the target value is a tensor of size (1,4), so the update has to happen for the chosen action\n",
    "        # taht is the action value update has to happen from the chosen action\n",
    "        target = q_t.clone().detach()\n",
    "        target[0,a_t] = y\n",
    "        batch_target.append(target)\n",
    "        batch_predict.append(q_t)\n",
    "        # apply batch training take care of uneven batches\n",
    "        j = (step+1)%batch_size\n",
    "        if (j == 0 and len(batch_predict) == batch_size) or r_t == -1:\n",
    "            target_tensor = torch.stack(batch_target)\n",
    "            predict_tensor = torch.stack(batch_predict)\n",
    "            loss = criteriation(predict_tensor,target_tensor)\n",
    "            l = loss.item()\n",
    "            r_h.append(r_t)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_predict.clear()\n",
    "            batch_target.clear()\n",
    "\n",
    "        #append the loss and reward for model train evaluation\n",
    "        if r_t == -20:\n",
    "            flag = -1\n",
    "        # to see significant changes in training take sample model from time to time\n",
    "        if(run == 0):\n",
    "            untrained_model = copy.deepcopy(model)\n",
    "        if(run == runs/2):\n",
    "            fifty_model = copy.deepcopy(model)\n",
    "        if(run == int(runs*3/4)):\n",
    "            seventy_five = copy.deepcopy(model)\n",
    "        step = step+1\n",
    "\n",
    "    loss_his.append(l)\n",
    "    reward_his.append(sum(r_h))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"RUNS\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.plot(reward_his)\n",
    "plt.title(\"REWARD\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2720f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"RUNS\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.plot(loss_his)\n",
    "plt.title(\"LOSS\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_val(model, game, runs):\n",
    "    max_vals = []\n",
    "    for _ in tqdm(range(runs), desc=\"Evaluating max tile\"):\n",
    "        game.initialise()\n",
    "        r_t = 0\n",
    "        m = []  # store all max tiles for this run\n",
    "        while r_t != -1:\n",
    "            s_t = game.matrix.copy()\n",
    "            q_t = model(encode_state(s_t))\n",
    "            a_t = eta_greedy(0.2, q_t)\n",
    "            r_t = game.run(a_t)\n",
    "            \n",
    "            # track max tile\n",
    "            max_tile = np.max(game.matrix)\n",
    "            \n",
    "            m.append(max_tile)\n",
    "        \n",
    "        # record the highest tile in this run\n",
    "        max_vals.append(max(m))\n",
    "\n",
    "    return max_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_un = max_val(untrained_model,game,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_max_tile_value = max_val(model,game,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b447b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"RUNS\")\n",
    "plt.ylabel(\"max tile value\")\n",
    "plt.plot(max_un,label = \"untrained\")\n",
    "plt.plot(trained_max_tile_value,label = \"Trained\")\n",
    "plt.title(\"Max_tile of run\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,game,run):\n",
    "    rewards = []\n",
    "    for _ in tqdm(range(run)):\n",
    "        game.initialise()\n",
    "        r_t = 0\n",
    "        ep_rewards = []\n",
    "        while r_t != -1:\n",
    "            s_t = game.matrix.copy()\n",
    "            q_t = model(encode_state(s_t))\n",
    "            a_t = eta_greedy(0.2, q_t)\n",
    "            r_t = game.run(a_t)\n",
    "            ep_rewards.append(r_t)\n",
    "        rewards.append(np.mean(ep_rewards))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58969027",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward_untrained = eval(untrained_model,game,100)\n",
    "fifty_model_average_reward = eval(fifty_model,game,100)\n",
    "seventy_five_average_reward = eval(seventy_five,game,100)\n",
    "reward = eval(model,game,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"RUNS\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(average_reward_untrained,label = \"0%\")\n",
    "plt.plot(fifty_model_average_reward,label = \"50%\")\n",
    "plt.plot(seventy_five_average_reward,label = \"75%\")\n",
    "plt.plot(reward,label = \"100%\")\n",
    "plt.title(\"Average Reward for different level of training\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ca2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20020bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
